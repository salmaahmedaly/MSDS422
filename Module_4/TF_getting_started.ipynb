{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the add defines the type of layers-- usually dense layers (what most ppl use)\n",
    "- flatten layer: for pictures for example when it has 6*7 rows and cols. FLATTEN will convert into a single row\n",
    "- dropout layer: randomly turning some nodes off  in each iteration(10%) to avoid over fitting/ over specialized. especially with complicated networks\n",
    "    \n",
    "# the number of nodes in each layer\n",
    "    - a general role, not more than 2x the input variables \n",
    "# type of activation transform \n",
    "    - RELU and SOFTMAX (cat)\n",
    "    - RELU and softplus are very similar\n",
    "    - RELU is turned off until 0 and then becomes linear\n",
    "    - SOFTplus has more of a curve\n",
    "\n",
    "#if it is input layer, then define dimensions of the input data\n",
    "# Compile - LOSS\n",
    "    - continuous target: MeanSquareError(), RootMeanSquareError()\n",
    "    - cat and binary: \n",
    "            - categorial cross entropy OR sparse cat cross entropy\n",
    "            - if the data is one hot encoded, use categorial cross entropy\n",
    "            - otherwise, use sparse\n",
    "    # Optimizer:\n",
    "            Adam() most used\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Specs\n",
    "theActivation = tf.keras.activations.tanh\n",
    "theLossMetric = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "theOptimizier = tf.keras.optimizers.Adam()\n",
    "theEpochs = 10 #iterations\n",
    "#first layer, number of nodes, activation functions, number of input nodes\n",
    "layer_01= tf.keras.layers.Dense(units=3, activation= theActivation, input_dim =2)\n",
    "#output layer\n",
    "layer_02= tf.keras.layers.Dense(units=1, activation= theActivation)\n",
    "\n",
    "model= tf.keras.Sequential()\n",
    "\n",
    "model.add(layer_01)\n",
    "model.add(layer_02)\n",
    "model.compile(loss= theLossMetric, optimizier= theOptimizier)\n",
    "\n",
    "\n",
    "# model.fit(x, y, theEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGRESSION\n",
      "A   = 4.003\n",
      "B   = -2.998\n",
      "Bias= 1.969\n",
      "X= [1 1]\n",
      "Y= 2.974\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a random train \n",
    "N = 1000\n",
    "\n",
    "X_List = []\n",
    "Y_List = []\n",
    "\n",
    "for i in range(N) :\n",
    "    A = random.randint( -10, 10 )\n",
    "    B = random.randint( -10, 10 )\n",
    "    Y_Val = 4*A - 3*B + 2 + random.normalvariate(0,1)\n",
    "    \n",
    "    X_List.append([A,B])\n",
    "    Y_List.append( Y_Val )\n",
    "\n",
    "X = np.array( X_List, dtype=\"f\" )\n",
    "Y = np.array( Y_List, dtype=\"f\" )\n",
    "\n",
    "\n",
    "#fitting x and y \n",
    "regModel = LinearRegression()\n",
    "regModel.fit( X, Y  )\n",
    "A = round( regModel.coef_[0], 3 )\n",
    "B = round( regModel.coef_[1], 3 )\n",
    "INTERCEPT = round( regModel.intercept_, 3 )\n",
    "\n",
    "print(\"REGRESSION\")\n",
    "print( \"A   =\", A )\n",
    "print( \"B   =\", B )\n",
    "print( \"Bias=\", INTERCEPT )\n",
    "\n",
    "#make predictions \n",
    "X_NEW = np.array( [[1,1]] )\n",
    "Y_NEW = regModel.predict( X_NEW )\n",
    "Y_NEW = np.round_( Y_NEW, 3 )\n",
    "print( \"X=\",X_NEW[0] )\n",
    "print( \"Y=\",Y_NEW[0] )\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15cd27b2dd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#solveing the same problem using tf \n",
    "theShapeSize = X.shape[1] #the number of input variables \n",
    "theActivation = tf.keras.activations.linear\n",
    "theLossMetric = tf.keras.losses.MeanSquaredError()\n",
    "theOptimizer = tf.keras.optimizers.Adam()\n",
    "theEpochs = 1000\n",
    "\n",
    "LAYER_01 = tf.keras.layers.Dense( units=1, activation=theActivation, input_dim=theShapeSize )\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add( LAYER_01 )\n",
    "model.compile( loss=theLossMetric,optimizer=theOptimizer)\n",
    "model.fit( X, Y, epochs=theEpochs, verbose=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR FLOW\n",
      "A   = 4.001\n",
      "B   = -2.998\n",
      "Bias= 1.967\n"
     ]
    }
   ],
   "source": [
    "W = LAYER_01.get_weights()\n",
    "print(\"TENSOR FLOW\")\n",
    "#not really sure what's happening but this gets the wts\n",
    "print( \"A   =\",round(W[0][0][0],3) )\n",
    "print( \"B   =\",round(W[0][1][0],3) )\n",
    "print( \"Bias=\",round(W[1][0]   ,3) )\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n",
      "X= [1 1]\n",
      "Y= [2.969]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_NEW = np.array( [[1,1]] )\n",
    "Y_NEW = model.predict( X_NEW )\n",
    "Y_NEW = np.round_( Y_NEW, 3 )\n",
    "print( \"X=\",X_NEW[0] )\n",
    "print( \"Y=\",Y_NEW[0] )\n",
    "print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4217bdb66c1619d3c7bf3a5f6dea8f7bd4200ce5b7b90b812037b80bad35d2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
